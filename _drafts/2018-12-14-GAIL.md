---
layout: post
title: Reading Notes on Imitation Learning - 1
modified: 2/29/2016, 9:00:24
tags: [math]
category: blog
comments: true
excerpt: "Notes on Apprentice Learning via IRL (Abbeel, Ng 2004), Maximum Entropy IRL (Ziebart et al. 2008), GAIL (Ho,
Ermon 2016)"
---

Imitation Learning (IL) Problem Formulation:
- What is IL?
- But exactly how much do we know about the underlying MDP?

Maximum Entropy IRL:
- Background:
    - reward is a linear combination of features; reward of trajectory is a sum of rewards in each state (not
        discounted); Thus, reward of trajectory is a linear combination of the sum of features of visited states.
    - we can get a empirical average feature count of expert demonstrations
    - MMP method (Ratliff et al. 2006) and the feature matching method proposed in (Abbeel, Ng 2004) both suffer from
        ambiguity of cost functions. 
    - In a sense, IRL problems are ill-posed because sometimes, there can be multiple optimal cost functions that
        satisfy the given trajectories. For example, in the case of degeneracy, the cost is 0 for all state-action pairs,
        and any trajectory is considered similarly optimal. Same problem happens in feature matching. If the demonstration
        in not optimal, then there might be multiple trajectories that can map to the same feature expectation.
    - We need a systematic way to break ties in the case of multiple optima.
- 

